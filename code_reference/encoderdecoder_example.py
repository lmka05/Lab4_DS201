# -*- coding: utf-8 -*-
"""EncoderDecoder_example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SfXiEuHJZ71ZWA5SMTBYMQIKJsFsNX-J
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

"""# KIẾN THỨC CƠ BẢN VỀ TENSOR SHAPES

Quy ước tensor shapes trong NLP:
- [batch_size, seq_len]: 2D tensor chứa các word indices

  VD: [[1, 5, 3, 2], [4, 6, 1, 2]] với batch_size=2, seq_len=4

- [batch_size, seq_len, embedding_dim]: 3D tensor chứa embeddings

  VD: shape [2, 4, 256] với batch_size=2, seq_len=4, embedding_dim=256

- [batch_size, vocab_size]: logits cho từ tiếp theo

  VD: shape [2, 5000] - xác suất của 5000 từ trong vocab

- [num_layers, batch_size, hidden_size]: hidden state từ LSTM

  VD: shape [1, 2, 128] với num_layers=1, batch_size=2, hidden_size=128

Lưu ý: batch_first=True → LSTM input: [batch_size, seq_len, feature_size]

# ENCODER-DECODER CƠ BẢN
"""

class Encoder(nn.Module):
    """
    ENCODER mẫu - 1 lớp LSTM

    Đọc toàn bộ câu, nén thành vector ngữ cảnh

    Cấu trúc:
        Input indices
          ↓ [Embedding Layer]
        Embedding vectors
          ↓ [LSTM]
        Hidden state (context vector)

    Ví dụ:
        Input: "the cat sat"
        Indices: [1, 5, 3]
        Shape: [batch_size=1, seq_len=3]
    """

    def __init__(self, vocab_size=5000, embedding_dim=128, hidden_size=128):
        """
        Args:
            vocab_size: số lượng từ trong vocab (VD: 5000)
            embedding_dim: chiều của embedding vector (VD: 128)
            hidden_size: chiều của hidden state của LSTM (VD: 128)
        """
        super(Encoder, self).__init__()

        # Layer 1: Embedding
        # Chuyển word indices (số nguyên) thành vector liên tục
        # Input: [batch_size, seq_len] → Output: [batch_size, seq_len, embedding_dim]
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,  # Bao nhiêu từ trong vocab
            embedding_dim=embedding_dim,  # Mỗi từ được biểu diễn bằng vector 128 chiều
            padding_idx=0  # Index 0 là <PAD> token
        )

        # Layer 2: LSTM
        # Xử lý tuần tự các embedding vectors
        # Input: [batch_size, seq_len, embedding_dim]
        # Output: [batch_size, seq_len, hidden_size] (nếu return_sequences=True)
        # Hidden/Cell: [num_layers, batch_size, hidden_size]
        self.lstm = nn.LSTM(
            input_size=embedding_dim,  # Kích thước input = embedding_dim
            hidden_size=hidden_size,   # Kích thước hidden state
            num_layers=1,              # Chỉ 1 lớp (1 lớp LSTM, bao nhiêu lớp tùy kiến trúc)
            batch_first=True           # Input shape: [batch_size, seq_len, features]
        )

    def forward(self, src):
        """
        Args:
            src: [batch_size, src_len] - indices của câu
                 VD: [[1, 5, 3, 2], [4, 6, 1, 2]] (batch_size=2)

        Returns:
            hidden: [num_layers=1, batch_size, hidden_size] - context vector
            cell: [num_layers=1, batch_size, hidden_size] - cell state

        Ví dụ follow:
            Input src: [2, 4] (2 câu, mỗi câu 4 từ)
            → Embedding: [2, 4, 128]
            → LSTM output: [2, 4, 128], hidden: [1, 2, 128]
        """

        # STEP 1: Embedding
        # Input: [batch_size, seq_len] → [batch_size, seq_len, embedding_dim]
        embedded = self.embedding(src)
        print(f"[Encoder] After embedding: {embedded.shape}")
        #         Output shape: [batch_size, seq_len, 128]

        # STEP 2: LSTM
        # Xử lý embeddings qua LSTM
        # - input: [batch_size, seq_len, embedding_dim]
        # - output: [batch_size, seq_len, hidden_size] (tất cả timesteps)
        # - hidden: [num_layers, batch_size, hidden_size] (timestep cuối)
        # - cell: [num_layers, batch_size, hidden_size] (cell state)
        output, (hidden, cell) = self.lstm(embedded)
        print(f"[Encoder] After LSTM - output: {output.shape}, hidden: {hidden.shape}")
        #         Output shapes:
        #         - output: [batch_size, seq_len, 128] (chứa hidden state ở tất cả timesteps)
        #         - hidden: [1, batch_size, 128] (hidden state ở timestep cuối)
        #         - cell: [1, batch_size, 128] (cell state ở timestep cuối)

        # STEP 3: Return hidden & cell state
        # Cái này sẽ dùng làm initial state cho Decoder
        return hidden, cell

class Decoder(nn.Module):
    """
    DECODER mẫu - 1 lớp LSTM (không attention)

    Cấu trúc:
        Từ trước (token)
          ↓ [Embedding Layer]
        Embedding vector
          ↓ [LSTM] (sử dụng hidden/cell từ Encoder)
        Output vector
          ↓ [Linear Layer]
        Logits của từ tiếp theo

    Ví dụ:
        Lúc t=0: Input <SOS> → Dự đoán từ đầu tiên
        Lúc t=1: Input từ đầu tiên → Dự đoán từ thứ 2
        ...
    """

    def __init__(self, vocab_size=5000, embedding_dim=128, hidden_size=128):
        """
        Args:
            vocab_size: số lượng từ trong từ vựng tiếng Việt
            embedding_dim: chiều của embedding vector
            hidden_size: chiều của hidden state
        """
        super(Decoder, self).__init__()

        # Layer 1: Embedding
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # Layer 2: LSTM
        # Tương tự như Encoder nhưng sử dụng initial state từ Encoder
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True
        )

        # Layer 3: Linear layer - dự đoán từ tiếp theo
        # Input: hidden_size → Output: vocab_size (logits cho mỗi từ)
        self.fc_out = nn.Linear(hidden_size, vocab_size)

    def forward(self, input_token, hidden, cell):
        """
        Args:
            input_token: [batch_size, 1] - 1 token (thường từ <SOS> hoặc từ trước)
                         VD: [[1], [1]] (batch_size=2, 1 token)
            hidden: [num_layers=1, batch_size, hidden_size] - từ Encoder
            cell: [num_layers=1, batch_size, hidden_size] - từ Encoder

        Returns:
            prediction: [batch_size, vocab_size] - logits của từ tiếp theo
            hidden: [num_layers=1, batch_size, hidden_size] - hidden state mới
            cell: [num_layers=1, batch_size, hidden_size] - cell state mới

        Ví dụ:
            input_token: [2, 1] (1 token cho 2 câu)
            → Embedding: [2, 1, 128]
            → LSTM: output [2, 1, 128], hidden [1, 2, 128]
            → Linear: [2, 5000] (logits)
        """

        # STEP 1: Embedding
        # Input: [batch_size, 1] → [batch_size, 1, embedding_dim]
        embedded = self.embedding(input_token)
        print(f"[Decoder] After embedding: {embedded.shape}")
        #         Output: [batch_size, 1, 128]

        # STEP 2: LSTM
        # Input: embedded [batch_size, 1, embedding_dim]
        # Initial state: hidden, cell từ Encoder (QUAN TRỌNG!)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        print(f"[Decoder] After LSTM - output: {output.shape}, hidden: {hidden.shape}")
        #         Output:
        #         - output: [batch_size, 1, hidden_size]
        #         - hidden: [1, batch_size, hidden_size]
        #         - cell: [1, batch_size, hidden_size]

        # STEP 3: Linear layer
        # Chuyển hidden state thành logits cho vocab
        # Input: [batch_size, 1, hidden_size] → Squeeze → [batch_size, hidden_size]
        #        → Linear → [batch_size, vocab_size]
        prediction = self.fc_out(output.squeeze(1))
        print(f"[Decoder] After linear: {prediction.shape}")
        #         Output: [batch_size, vocab_size]

        return prediction, hidden, cell

class Seq2SeqSimple(nn.Module):
    """
    MÔ HÌNH SEQ2SEQ mẫu

    Kết hợp Encoder + Decoder
    - Encoder đọc câu nguồn toàn bộ
    - Decoder sinh câu dịch từng bước, có thể dùng teacher forcing
    """

    def __init__(self, encoder, decoder):
        super(Seq2SeqSimple, self).__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, src, tgt, teacher_forcing_ratio=0.5):
        """
        Args:
            src: [batch_size, src_len] - câu tiếng Anh
                 VD: [[1, 5, 3, 2], [4, 6, 1, 2]]
            tgt: [batch_size, tgt_len] - câu tiếng Việt (có <SOS> ở đầu)
                 VD: [[1, 7, 8, 9, 2], [1, 10, 11, 2, 0]]
            teacher_forcing_ratio: xác suất dùng từ đúng thay vì từ dự đoán
                                  - 0.5 = 50% dùng từ đúng, 50% dùng dự đoán

        Returns:
            outputs: [batch_size, tgt_len-1, vocab_size] - logits của tất cả timesteps
        """
        batch_size = src.shape[0]
        tgt_len = tgt.shape[1]
        vocab_size = self.decoder.fc_out.out_features

        # STEP 1: Encoder - đọc câu nguồn
        print("\n=== ENCODER ===")
        hidden, cell = self.encoder(src)
        print(f"Encoder outputs - hidden: {hidden.shape}, cell: {cell.shape}")

        # STEP 2: Decoder - sinh câu dịch
        print("\n=== DECODER (Auto-regressive) ===")
        decoder_input = tgt[:, 0].unsqueeze(1)  # Lấy token <SOS> từ đầu
        outputs = []

        # Sinh từng từ trong câu dịch
        for t in range(1, tgt_len):
            print(f"\nTimestep {t}:")
            print(f"  Input: {decoder_input.shape}")

            # Sinh dự đoán cho timestep này
            prediction, hidden, cell = self.decoder(decoder_input, hidden, cell)
            outputs.append(prediction.unsqueeze(1))
            print(f"  Prediction: {prediction.shape} (logits)")

            # TEACHER FORCING
            # - Nếu dùng teacher forcing (dự đoán đúng): lấy từ đúng từ tgt
            # - Ngược lại: lấy từ mô hình dự đoán tốt nhất

            if np.random.random() < teacher_forcing_ratio:
                decoder_input = tgt[:, t].unsqueeze(1)  # Dùng từ đúng
                print(f"  → Teacher forcing: input timestep {t+1} = target từ")
            else:
                top_token = torch.argmax(prediction, dim=1)  # Lấy từ tốt nhất
                decoder_input = top_token.unsqueeze(1)
                print(f"  → Greedy: input timestep {t+1} = predicted từ")

        # Ghép tất cả predictions
        outputs = torch.cat(outputs, dim=1)  # [batch_size, tgt_len-1, vocab_size]
        print(f"\nFinal outputs shape: {outputs.shape}")

        return outputs

"""# BAHDANAU ATTENTION"""

class BahdanauAttention(nn.Module):
    """
    Bahdanau Attention (Additive Attention)

    Công thức:
    score(s_t, h_i) = v^T * tanh(W_q * s_t + W_k * h_i)
    """
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.hidden_size = hidden_size

        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)  # Query projection
        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)  # Key projection
        self.v = nn.Linear(hidden_size, 1, bias=False)  # Scoring function

    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: [batch_size, hidden_size] - s_t
            encoder_outputs: [batch_size, src_len, hidden_size] - {h_i}

        Returns:
            context: [batch_size, hidden_size]
            attention_weights: [batch_size, src_len]
        """
        # Tính scores: score = v^T * tanh(W_q * s_t + W_k * h_i)
        query = self.W_q(decoder_hidden).unsqueeze(1)  # [batch_size, 1, hidden_size]
        keys = self.W_k(encoder_outputs)  # [batch_size, src_len, hidden_size]

        # Additive attention
        energy = torch.tanh(query + keys)  # [batch_size, src_len, hidden_size]
        scores = self.v(energy).squeeze(2)  # [batch_size, src_len]

        # Softmax
        attention_weights = torch.softmax(scores, dim=1)  # [batch_size, src_len]

        # Weighted sum
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)
        # [batch_size, 1, src_len] x [batch_size, src_len, hidden_size] -> [batch_size, hidden_size]

        return context, attention_weights


class DecoderAttention(nn.Module):
    """
    Decoder với Bahdanau Attention cho Bài 2
    """
    def __init__(self, vocab_size, embedding_dim=256, hidden_size=256,
                 num_layers=5, dropout=0.3):
        super(DecoderAttention, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.attention = BahdanauAttention(hidden_size)

        # Output: kết hợp LSTM output với context vector
        self.fc = nn.Linear(hidden_size * 2, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input_token, hidden, cell, encoder_outputs):
        """
        Args:
            input_token: [batch_size, 1]
            hidden: [num_layers, batch_size, hidden_size]
            cell: [num_layers, batch_size, hidden_size]
            encoder_outputs: [batch_size, src_len, hidden_size]

        Returns:
            prediction: [batch_size, vocab_size]
            hidden, cell: state mới
            attention_weights: [batch_size, src_len]
        """
        embedded = self.dropout(self.embedding(input_token))  # [batch_size, 1, embedding_dim]

        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        # output: [batch_size, 1, hidden_size]

        # Attention
        decoder_hidden = hidden[-1]  # Lấy hidden state lớp cuối
        context, attention_weights = self.attention(decoder_hidden, encoder_outputs)

        # Kết hợp output với context
        combined = torch.cat([output.squeeze(1), context], dim=1)  # [batch_size, hidden_size*2]
        prediction = self.fc(combined)  # [batch_size, vocab_size]

        return prediction, hidden, cell, attention_weights

"""# LUONG ATTENTION"""

class LuongAttention(nn.Module):
    """
    Luong Attention (Multiplicative Attention)

    Công thức:
    score(s_t, h_i) = s_t^T * W * h_i
    """
    def __init__(self, hidden_size):
        super(LuongAttention, self).__init__()
        self.W = nn.Linear(hidden_size, hidden_size, bias=False)

    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: [batch_size, hidden_size]
            encoder_outputs: [batch_size, src_len, hidden_size]

        Returns:
            context: [batch_size, hidden_size]
            attention_weights: [batch_size, src_len]
        """
        # Luong: score = s_t^T * W * h_i
        query = decoder_hidden.unsqueeze(1)  # [batch_size, 1, hidden_size]
        keys = self.W(encoder_outputs)  # [batch_size, src_len, hidden_size]

        # Multiplicative attention
        scores = torch.bmm(query, keys.transpose(1, 2)).squeeze(1)  # [batch_size, src_len]

        # Softmax
        attention_weights = torch.softmax(scores, dim=1)

        # Weighted sum
        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)

        return context, attention_weights